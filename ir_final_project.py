# -*- coding: utf-8 -*-
"""IR_Final_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11O2Z_lJqDFoP9HudLaewlMRPCHCNgPym
"""

import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt

# Load the CSV file into a DataFrame
dataset_csv_file = "/content/condition.csv"
data = pd.read_csv(dataset_csv_file)
data.head()

Graph = nx.DiGraph()     # initializing the graph


for index, row in data.iterrows():    #nodes and edges added to directed graph
    condition_id = row['id']
    disease_name = row['name']
    source_id = row['source_id']
    url = row['url']

    Graph.add_node(condition_id, name=disease_name, source_id=source_id, url=url)   #addition of new node

    if pd.notnull(source_id):           #source id exist? addition of edge
        Graph.add_edge(source_id, condition_id)

position = nx.spring_layout(Graph)  # positions for all nodes , visualizing the graph

# Drawing nodes and edges
nx.draw_networkx_nodes(Graph, position, node_size=700)
nx.draw_networkx_edges(Graph, position, width=2)

# Drawing labels
nx.draw_networkx_labels(Graph, position, font_size=10, font_family="sans-serif")

# Displaying the graph
plt.axis('off')
plt.show()

import csv
import networkx as nx

# Load knowledge graph from CSV
def load_graph_from_csv(file_path):
    knowledge_graph = nx.Graph()    #creating graph object
    with open(file_path, 'r') as file:
        reader = csv.reader(file)
        for row in reader:
            if len(row) == 4:  # Assuming format: id, name, source_id, url
                condition_id, disease_name, source_id, url = row
                knowledge_graph.add_node(condition_id, name=disease_name, source_id=source_id, url=url)# Add a node to the graph representing the disease or condition

                if source_id:
                    knowledge_graph.add_edge(source_id, disease_name)
    return knowledge_graph

# Calculating Jaccard similarity coefficient
def Jaccard_similarity(query, document_name):
    Query_Tokens = set(query.lower().split())
    Document_Tokens = set(document_name.lower().split())  #convertig text to lowercase and split into token
    intersection = Query_Tokens.intersection(Document_Tokens)    #finding common elements
    union = Query_Tokens.union(Document_Tokens)                #combining unique elements
    return len(intersection) / len(union) if union else 0

# Retrieve top 5 related documents from knowledge graph
def retrieve_top_related_documents(query, knowledge_graph, top_n=5):
    Related_Documents = []  #create list for related documents
    for node, data in knowledge_graph.nodes(data=True):
        document_name = data.get('name', '')
        document_url = data.get('url', '')
        similarity = Jaccard_similarity(query, document_name)   # Calculate Jaccard similarity between query and document name for explainibility
        Related_Documents.append((document_name, similarity, document_url))
    Related_Documents.sort(key=lambda x: x[1], reverse=True)    #sorting the list
    return Related_Documents[:top_n]

# Example of our code
File_Path = "/content/condition.csv"

Knowledge_Graph = load_graph_from_csv(File_Path)

User_Query = input("Enter your query: ")   # user inputs their query
Related_Documents = retrieve_top_related_documents(User_Query, Knowledge_Graph)

print("\nTop 5 Related Documents:")
if Related_Documents:
    for i, (document, score, url) in enumerate(Related_Documents, start=1):
        print( f"{i}. Document: {document}, Similarity Score: {score:.2f}, URL: {url}")
else:
    print("No related documents found.")